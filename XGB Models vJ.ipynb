{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Models\n",
    "\n",
    "The Data will be split by week and use week 9 as a validation set \n",
    "\n",
    "The Workflow will be as follow: \n",
    "\n",
    "    ** Use the default xgbregresor to train and plot feature importance\n",
    "    ** Use a gridsearch to find the ideal hyperparameters (with only 50 estimators) \n",
    "    ** Train using all data and 500 estimators as our final model \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_w11 = False #I'm going to use this variable to know when to use w10 for training\n",
    "dataset_file = 'jorge_dataset_modifiedwPred.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\computation\\__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import LabelKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load computed Dataset\n",
    "def load_dataset(folder = ''):\n",
    "    \n",
    "    \n",
    "    filename = dataset_file\n",
    "    filepath = os.path.join(folder,filename)        \n",
    "    \n",
    "    df_train = pd.read_csv(filepath,                           \n",
    "                           dtype  = {'Producto_ID':'int32',\n",
    "                                     'Semana':'int8',\n",
    "                                     'Cliente_ID':'int32',\n",
    "                                     'Agencia_ID':'uint16',\n",
    "                                     'Canal_ID':'int8',\n",
    "                                     'Ruta_SAK':'int32',\n",
    "                                     'ZipCode':'int16',                                     \n",
    "                                     'Mean2':'float32','Mean3':'float32','Mean4':'float32',\n",
    "                                     'Mean6':'float32','Mean7':'float32',\n",
    "                                     'Median2':'float32','Median3':'float32','Median4':'float32',\n",
    "                                     'Median6':'float32','Median7':'float32',\n",
    "                                     'Last_per_Ruta_SAK':'float32','Last_per_Cliente_ID':'float32',\n",
    "                                     'Demanda_uni_equil':'float32'})\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Unnamed: 0', u'Agencia_ID', u'Canal_ID', u'Cliente_ID',\n",
      "       u'Demanda_uni_equil', u'Producto_ID', u'Ruta_SAK', u'Semana',\n",
      "       u'ZipCode', u'id', u'Last_per_Cliente_ID', u'Last_per_Ruta_SAK',\n",
      "       u'week_ct', u'Log_Target_mean_lag1', u'Log_Target_mean_lag2',\n",
      "       u'Log_Target_mean_lag3', u'Log_Target_mean_lag4', u'Lags_sum', u'brand',\n",
      "       u'Qty_Ruta_SAK_Bin', u'num_prod', u'num_prod_uni'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Demanda_uni_equil</th>\n",
       "      <th>Producto_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Semana</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>week_ct</th>\n",
       "      <th>Log_Target_mean_lag1</th>\n",
       "      <th>Log_Target_mean_lag2</th>\n",
       "      <th>Log_Target_mean_lag3</th>\n",
       "      <th>Log_Target_mean_lag4</th>\n",
       "      <th>Lags_sum</th>\n",
       "      <th>brand</th>\n",
       "      <th>Qty_Ruta_SAK_Bin</th>\n",
       "      <th>num_prod</th>\n",
       "      <th>num_prod_uni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>15766</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1212</td>\n",
       "      <td>3301</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>15766</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1216</td>\n",
       "      <td>3301</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>15766</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1238</td>\n",
       "      <td>3301</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>15766</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1240</td>\n",
       "      <td>3301</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1110</td>\n",
       "      <td>7</td>\n",
       "      <td>15766</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1242</td>\n",
       "      <td>3301</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Agencia_ID  Canal_ID  Cliente_ID  Demanda_uni_equil  \\\n",
       "0           0        1110         7       15766           1.386294   \n",
       "1           1        1110         7       15766           1.609438   \n",
       "2           2        1110         7       15766           1.609438   \n",
       "3           3        1110         7       15766           1.609438   \n",
       "4           4        1110         7       15766           1.386294   \n",
       "\n",
       "   Producto_ID  Ruta_SAK  Semana  ZipCode   id      ...       week_ct  \\\n",
       "0         1212      3301       3     2008  0.0      ...           3.0   \n",
       "1         1216      3301       3     2008  0.0      ...           3.0   \n",
       "2         1238      3301       3     2008  0.0      ...           3.0   \n",
       "3         1240      3301       3     2008  0.0      ...           3.0   \n",
       "4         1242      3301       3     2008  0.0      ...           3.0   \n",
       "\n",
       "   Log_Target_mean_lag1  Log_Target_mean_lag2  Log_Target_mean_lag3  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   Log_Target_mean_lag4  Lags_sum  brand  Qty_Ruta_SAK_Bin  num_prod  \\\n",
       "0                   0.0       0.0    2.0               1.0      23.0   \n",
       "1                   0.0       0.0    2.0               1.0      23.0   \n",
       "2                   0.0       0.0    2.0               1.0      23.0   \n",
       "3                   0.0       0.0    2.0               1.0      23.0   \n",
       "4                   0.0       0.0    2.0               1.0      23.0   \n",
       "\n",
       "   num_prod_uni  \n",
       "0          23.0  \n",
       "1          23.0  \n",
       "2          23.0  \n",
       "3          23.0  \n",
       "4          23.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                int64\n",
      "Agencia_ID               uint16\n",
      "Canal_ID                   int8\n",
      "Cliente_ID                int32\n",
      "Demanda_uni_equil       float32\n",
      "Producto_ID               int32\n",
      "Ruta_SAK                  int32\n",
      "Semana                     int8\n",
      "ZipCode                   int16\n",
      "id                      float64\n",
      "Last_per_Cliente_ID     float32\n",
      "Last_per_Ruta_SAK       float32\n",
      "week_ct                 float64\n",
      "Log_Target_mean_lag1    float64\n",
      "Log_Target_mean_lag2    float64\n",
      "Log_Target_mean_lag3    float64\n",
      "Log_Target_mean_lag4    float64\n",
      "Lags_sum                float64\n",
      "brand                   float64\n",
      "Qty_Ruta_SAK_Bin        float64\n",
      "num_prod                float64\n",
      "num_prod_uni            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df.columns\n",
    "display(df.head())\n",
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Since I use Last -> Discard Week 3\n",
    "\n",
    "#### Test data is week 10 and 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "validation = 9 \n",
    "\n",
    "if predict_w11:\n",
    "    validation = 10\n",
    "\n",
    "df.Demanda_uni_equil[df.Demanda_uni_equil < 0] = 0\n",
    "df_validation = df[df.Semana == validation]\n",
    "df_train = df[(df.Semana != 3) & (df.Semana < validation)]\n",
    "df_test = df[df.Semana > validation]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Columns that are not features \n",
    "#drop_columns = ['id','Semana','Demanda_uni_equil','DemandaAgencia_ID','DemandaCanal_ID','DemandaRuta_SAK','DemandaZipCode']\n",
    "drop_columns = ['id','Semana','Demanda_uni_equil']\n",
    "\n",
    "model_002 = ['Mean3','Median3', 'Mean2', 'Median2','Mean4','Median4', 'Mean6', 'Median6','Mean7','Median7']\n",
    "model_003 = ['ZipCode','Cliente_ID','Agencia_ID','Ruta_SAK','Producto_ID','Canal_ID','DemandaRuta_SAK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Semana = df_train.Semana\n",
    "\n",
    "features = df_train.drop(drop_columns,axis=1).columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance \n",
    "\n",
    "### Lets train a default model and see the feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBRegressor(seed=1234)\n",
    "#%time gbm.fit(X_train, Y_train, eval_metric='rmse', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 10))\n",
    "#xgb.plot_importance(gbm.booster())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning (YaY!) \n",
    "\n",
    "#### Cross-Validation and GridSearch\n",
    "     Since I'm using xgboost I'm going to use the sklearn API so I can use GridSearchCV\n",
    "          \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n",
    "\n",
    "    Also, Since this is not just normal data where each row is a data point but each point correspond to a point in a sequence (Weekly delivers). I'm going to use the 'Semana' to create the folds, for this refer to sklearn Label KFold \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.LabelKFold.html#sklearn.cross_validation.LabelKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch \n",
    "\n",
    "    Pretty common grid \n",
    "    \n",
    "        * Max depth of the tree\n",
    "        * n estimators: # Trees\n",
    "        * Learning Rate\n",
    "        * subsample\n",
    "        * alpha and lambda regs\n",
    "        \n",
    "please refer:\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':[10,6], \n",
    "        'n_estimators':[50] ,\n",
    "        'learning_rate':[0.1,0.05],         \n",
    "         'subsample': [0.5,1],\n",
    "         #'reg_alpha':[0,1], #L2 term\n",
    "         #'reg_lambda':[0,1]#, #L1 tem            \n",
    "         #'silent': [False]\n",
    "        }\n",
    "\n",
    "best_param = {'max_depth':[10], \n",
    "        'n_estimators':[100] ,\n",
    "        'learning_rate':[0.1],         \n",
    "         'subsample': [0.5],\n",
    "         #'reg_alpha':[0,1], #L2 term\n",
    "         #'reg_lambda':[0,1]#, #L1 tem            \n",
    "         #'silent': [False]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried the gridsearch with CV, takes too long in my machine (4Cores 28GB) I gave up at 12hrs running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO! \n",
    "###### and probably get a coffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c1dd26814783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgrid_gbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mgrid_gbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for this combination: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_gbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevals_result_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "grid_scores_ = pd.DataFrame(columns=['max_depth','learning_rate','n_estimators','subsample','score' ])\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "for g in ParameterGrid(best_param):\n",
    "    start = time.time()\n",
    "    \n",
    "    grid_gbm = xgb.XGBRegressor(**g)\n",
    "    grid_gbm.fit(X_train, Y_train,eval_set=[(X_valid,Y_valid)] ,eval_metric='rmse', early_stopping_rounds=10)\n",
    "    \n",
    "    print \"Score for this combination: \", grid_gbm.evals_result_.values()[-1].values()[-1][-1]\n",
    "    g['rmse'] = grid_gbm.evals_result_.values()[-1].values()[-1][-1] #I'm sure there must be a better way to get this, but this nested dict is annoying\n",
    "    grid_scores_ = grid_scores_.append(g, ignore_index=True)\n",
    "            \n",
    "    print(\"total time taken this loop: \", time.time() - start)\n",
    "\n",
    "    \n",
    "    \n",
    "display(grid_scores_)    \n",
    "#print \"OOB: %0.5f\" % best_score \n",
    "#print \"Grid:\", best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Retrain model with best params and test RMSLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReTrain the Model with all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = X_train.append(X_valid, ignore_index=True)\n",
    "#Y = Y_train.append(Y_valid)\n",
    "df_train = df_train.append(df_validation, ignore_index=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "for g in ParameterGrid(best_param):\n",
    "    start = time.time()\n",
    "    X = df_train[features]\n",
    "    Y = df_train['Demanda_uni_equil']\n",
    "    gbm = xgb.XGBRegressor(**g)\n",
    "    gbm.fit(X, Y ,eval_metric='rmse',early_stopping_rounds=10)\n",
    "       \n",
    "            \n",
    "    print(\"total time taken this loop: \", time.time() - start)\n",
    "\n",
    "pred = gbm.predict(X)\n",
    "\n",
    "print \"R2 = \",r2_score(np.float64(Y),np.float64(pred))\n",
    "print \"RMSE\", np.sqrt(mean_squared_error(np.float64(Y),np.float64(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "gbm._Booster.save_model('xgb_bimbo.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Submission  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For week 10 the prediction is easy, use the inputs normally from week 9 \n",
    "\n",
    "For Week 11 the prediction is more complicated because we need to update the following features:\n",
    "\n",
    "    -Last_per_Cliente: From week 10 (predicted)\n",
    "    -Last_per_Route: From week 10 (predicted) \n",
    "    \n",
    "For Last I have to run the code again    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_Last(df ,clusters = ['Cliente_ID']):\n",
    "    \n",
    "    \n",
    "    validclusters = np.intersect1d(clusters, df.columns)\n",
    "    not_valid_clusters = np.setdiff1d(clusters, df.columns)\n",
    "    \n",
    "    for non in not_valid_clusters:\n",
    "        \n",
    "        print \"Cluster \", non, \" not found in the dataset\"            \n",
    "    \n",
    "    drop_columns = ['Venta_uni_hoy','Venta_hoy','Dev_uni_proxima','Dev_proxima']\n",
    "    try:\n",
    "        df = df.drop(drop_columns, 1) # Try to drop the columns if the dataset has them\n",
    "    except:\n",
    "        df = df # no drop \n",
    "    \n",
    "    for cluster in validclusters:\n",
    "        \n",
    "        print 'Working on cluster:', cluster                                                    \n",
    "        \n",
    "        target_col = 'Last_per_'+cluster\n",
    "        group = [cluster,'Producto_ID','Semana']\n",
    "                    \n",
    "        try:\n",
    "            df = df.drop(target_col,1) # If the target exist then remove it\n",
    "        except:\n",
    "            df = df \n",
    "        \n",
    "        # First Compute the Mean by grouping the Cluster-Producto per Week Demand \n",
    "        # Then shift by Product_ID - we want to treat each products independent \n",
    "        # Lastly reset the index because we don't wanna change the dataset\n",
    "        shifted_df = ( df.groupby(group).agg({'Demanda_uni_equil': np.mean})['Demanda_uni_equil'] \\\n",
    "                                         .groupby(level=1).shift(1) \\\n",
    "                                        .reset_index())\n",
    "        \n",
    "        shifted_df = shifted_df.fillna(0)   # Fill NaN with 0 \n",
    "        shifted_df['Demanda_uni_equil'] = np.float32(shifted_df['Demanda_uni_equil']) #reset index resets the dtypes\n",
    "        shifted_df.rename(columns={'Demanda_uni_equil': 'Last_per_'+cluster }, inplace=True) # rename cols        \n",
    "        df = df.merge(shifted_df, on=group)\n",
    "        \n",
    "        #Note to self: break the line using \\                               \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_train.dtypes\n",
    "print df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concat df_test and train \n",
    "df = df_train.append(df_validation, ignore_index=True).append(df_test,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict week 10\n",
    "features = X.columns\n",
    "if (~predict_w11):\n",
    "    df['Demanda_uni_equil'][df.Semana == 10] = gbm.predict(df[features][df.Semana ==10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df['Demanda_uni_equil'][df.Semana == 10].isnull().any()\n",
    "display(df['Demanda_uni_equil'][df.Semana == 10].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pablo_lag(df):\n",
    "    semana_client_prod_mean = df.groupby(['Semana','Cliente_ID','Producto_ID'],as_index=False).agg({'Demanda_uni_equil': 'mean'})\n",
    "\n",
    "    #here we add the number of lags we want\n",
    "    lag=4\n",
    "    target_col = 'Demanda_uni_equil'\n",
    "    for i in range(1,lag+1):\n",
    "        semana_client_prod_mean['Semana'] += 1\n",
    "        semana_client_prod_mean.rename(columns={target_col: 'Log_Target_mean_lag%d' %(i)}, inplace=True)  \n",
    "        \n",
    "        try:\n",
    "            df = df.drop('Log_Target_mean_lag%d' %(i),1) # If the target exist then remove it\n",
    "        except:\n",
    "            df = df \n",
    "            \n",
    "        df = pd.merge(df,semana_client_prod_mean, how = 'left', on = ['Semana','Cliente_ID','Producto_ID'])\n",
    "        df['Log_Target_mean_lag%d' %(i)].fillna(0, inplace=True) # we replace the client-product log mean NaN/Not found on the week before with ZERO\n",
    "        target_col = 'Log_Target_mean_lag%d' %(i)\n",
    "        #data = data[data.Semana != i+2] # here we delete the week rows we dont have lags for\n",
    "        \n",
    "    return df\n",
    "def pablo_lag_sum(df):\n",
    "        #We want to sum the lags up until week 9, this means that we need to sum lag2 and up.\n",
    "    df['Lags_sum'] = 0\n",
    "    for i in range(1,lag+1):\n",
    "        df['Lags_sum'] += df['Log_Target_mean_lag%d' %(i)]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def Lu_unique_product(df):\n",
    "        # for every client, get the number of products ordered in the current week\n",
    "    df_num_prods = df.groupby(['Semana','Cliente_ID'])['Producto_ID'].apply(lambda x: len(x)).reset_index()\n",
    "    df_num_prods.rename(columns={df_num_prods.columns[-1]: 'num_prod'}, inplace=True)\n",
    "\n",
    "\n",
    "    # for every client, get the number of unique products ordered in the current week\n",
    "    df_num_prods_unique = df.groupby(['Semana','Cliente_ID'])['Producto_ID'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    df_num_prods_unique.rename(columns={df_num_prods_unique.columns[-1]: 'num_prod_uni'}, inplace=True)\n",
    "\n",
    "    # merge to the dataset\n",
    "    df = df.merge(df_num_prods,on=['Semana','Cliente_ID'])\n",
    "    df = df.merge(df_num_prods_unique, on=['Semana','Cliente_ID'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df =df.drop(model_002,1)\n",
    "# Re Run Last Code\n",
    "df = add_Last(df,['Cliente_ID','Ruta_SAK'])\n",
    "\n",
    "# Re Run Lagged Code \n",
    "lag=4\n",
    "df = pablo_lag(df)\n",
    "\n",
    "# Re Run Lag Sum Code\n",
    "df = pablo_lag_sum(df)\n",
    "\n",
    "# Re Run Unique Product \n",
    "df = Lu_unique_product(df)\n",
    "\n",
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do I Re Train with week 10 Predictions to make a more accurate week 11 predictions? \n",
    "\n",
    "### TODO\n",
    "\n",
    "Do I have weeks with demand = 0 ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict week 11\n",
    "df['Demanda_uni_equil'][df.Semana == 11] = gbm.predict(df[features][df.Semana ==11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take only the last two weeks\n",
    "df_submit = df[df.Semana >9]\n",
    "# Revert the log1p to the demand  \n",
    "df_submit.Demanda_uni_equil[df_submit.Demanda_uni_equil < 0] = 0 # Set Negative Values to 0\n",
    "df_submit.Demanda_uni_equil = df_submit.Demanda_uni_equil.apply(lambda x: np.expm1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(df_submit[['id','Demanda_uni_equil']][df_submit.Semana==10].describe())\n",
    "display(df_submit[['id','Demanda_uni_equil']][df_submit.Semana==11].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many are negative ? \n",
    "\n",
    "df_submit[(df_submit.Semana == 10) & (df_submit.Demanda_uni_equil <= 0)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save csv with sumission data \n",
    "df_submit[['id','Demanda_uni_equil']].to_csv('submit_model_j006.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
